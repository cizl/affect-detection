% Paper template for TAR 2018
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2018}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Making your deep neural network's life burden free}

\name{David Lozić, Luka Markušić} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{david.lozic,luka.markusic\}@fer.hr}\\
}
          
         
\abstract{ 
This document provides the instructions on formatting the TAR system description paper in \LaTeX{}. This is where you write the abstract (i.e., summary) of the work you carried out within the project. The abstract is a paragraph of text ranging between 70 and 150 words.
}

\begin{document}

\maketitleabstract

\section{Introduction}

Sentiment analysis might be more commonly known by its flashier and broader title, \textit{opinion mining}. 
Generally speaking, the task of sentiment analysis is to identify and categorize the writer's feelings and/or opinions into several categories. 
It has become critical for companies to see how customer's opinions change with the passage of time and trends. 
In the academia, the very popular SemEval series hosts many sentiment analysis competitions annually, with many different subtasks.

In recent years, the trend seems to follow deep learning models,
as they often occupy the top positions on the leaderboards.
At the very top of these competitions many elaborate models 
and complex ensambles can be found
which often provide a minor boost in performance.
Given that these models provide satisfactory performance,
we are interested in finding the minimal working model architecture
which still offers acceptable results.
The motivation behind this is to find the optimal industry oriented
model which would work well in a production environment.

This is made possible due to the rather small size of the dataset,
which provides us with the unique opportunity to do a detailed
architecture and hyperparameter analysis.
In essence, we perform an architecture and hyperparameter grid
search in pursuit of a lightweight model.

\section{Related work}
A substantial number of approaches rely greatly on an underlying sentiment lexicon \citep{lexicon_paper} which, while being one of the most robust, is one of the most time-consuming methods. The trend regarding models based on word representations as vectors of real numbers \citep{w2v} shows no inclination to slow down. Our main source of inspiration was EiTAKA's system \citep{mohammed-semeval} for SemEval-2018. The task their model was suited for was comprised of several different problems:

\textbf{EI-reg:} given a tweet and an emotion, determine the intensity of that emotion as a score between 0 and 1.

\textbf{EI-oc:} given a tweet and an emotion, classify the tweet into one of four ordinal classes of intensity.

\textbf{V-reg:} given a tweet, determine the valence of a tweet as a score between 0 and 1.

\textbf{V-oc:} given a tweet, classify it into one of seven ordinal classes.


Their work was focused towards reaching the best results, as for our proposed model builds upon their work, trying to strip down all the unnecessary material while preserving performance.

\section{Resources}
The dataset for all of the tasks was provided by SemEval itself \citep{mohammed-semeval}, both in English, Spanish and Arabic, our language of choice was English. Additional resources in form of word embeddigs and sentiment lexicons were provided by Stanford \citep{glove} and National Research Council in Canada \citep{nrcic}, respectively. 


\section{System Description}
In this section we present how our system performs the grid search.
The system has been implemented using the Keras deep learning library \citep{chollet2015keras}.

\subsection{Preprocessing}
For text preprocessing, we use the tokenizer tool provided in the Keras library.
The tweets are lowercased and the special characters and
punctuations are removed.
These characters include: \textit{!"#\$\%&()*+,-./:;<=>?@[\textbackslash]\^\_`\{|\}~}.

After this normalization step, the tweets are tokenized using the Keras tokenizer.

\subsection{Input}
The words from the preprocessing step are mapped to word embeddings
and affect embeddings.
For word embeddings, we use the GloVe word vectors \citep{glove} 
which embed words into 200 dimensional vectors.
The affect embedding map words to the affect values which are
extracted from the NRC affect lexicon \citep{mohammed-semeval}.
These embeddings produce 4 dimensional word vectors where every
dimension represents an affect intensity value for the following emotions: \textit{anger, fear, joy, sadness}.
Finally, the two embeddings are concatenated into a single 204 dimension embedding.
These embeddings represent the model input.

\subsection{Architecture grid}
The system performs hyperparameter search for all architectures
given by the user. 
An architecture is defined by a list of layers from which
the model is built using the \textit{Sequential} Keras API.
The system fills every layer of the architecture with
arguments generated from the hyperparameter grid.
The sequential list structure of architectures does not
allow for more complex models with multiple inputs that can be
compiled using the \textit{Functional} Keras API.
But this was not our goal since we are searching for minimal models, 
although the system could be easily modified to satisfy such a need.

\subsection{Hyperparameter grid}
To perform hyperparameter search, the user needs to define
a list of possible values for chosen parameters of every type
of layer which is present in the architecture.
The system creates a Cartesian product of hyperparameters
which are used to fill the layers of the specified architecture.

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{images/architecture.pdf}
\caption{Model pipeline with an example architecture.}
\label{fig:figure1}
\end{center}
\end{figure}


\subsection{Training}

\section{Results}

\begin{table}
\caption{Task 1.a results (emotion intensity regression)}
\label{tab:narrow-table}
\begin{center}
\begin{tabular}{lllll}
\toprule
& joy & anger & fear & sadness \\
\midrule
EiTAKA & 0.723  & 0.704 & 0.715 & 0.731 \\
Best Convnet & & & & \\
Small Convnet & 0.627 & 0.603 & 0.612 & 0.621 \\
SVM unigrams & 1.526 & 0.525 & 0.575 & 0.453 \\
Random baseline & -0.018 & 0.024 & -0.058 & 0.020 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}
This concludes the paper.

\bibliographystyle{tar2018}
\bibliography{tar2018} 

\end{document}

