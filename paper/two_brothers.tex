% Paper template for TAR 2018
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2018}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Supercharged deep learning for a happier life}

\name{David Lozić, Luka Markušić} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{david.lozic,luka.markusic\}@fer.hr}\\
}
          
         
\abstract{ 
This document provides the instructions on formatting the TAR system description paper in \LaTeX{}. This is where you write the abstract (i.e., summary) of the work you carried out within the project. The abstract is a paragraph of text ranging between 70 and 150 words.
}

\begin{document}

\maketitleabstract

\section{Introduction}

Sentiment analysis might be more commonly known by its flashier and broader title, \textit{opinion mining}. 
Generally speaking, the task of sentiment analysis is to identify and categorize the writer's feelings and/or opinions into several categories. 
It has become critical for companies to see how customer's opinions change with the passage of time and trends. 
In the academia, the very popular SemEval series hosts many sentiment analysis competitions annually, with many different subtasks.

In recent years, the trend seems to follow deep learning models,
as they often occupy the top positions on the leaderboards.
At the very top of these competitions many elaborate models 
and complex ensambles can be found
which often provide a minor boost in performance.
Given that these models provide satisfactory performance,
we are interested in finding the minimal working model architecture
which still offers acceptable results.
The motivation behind this is to find the optimal industry oriented
model which would work well in a production environment.

This is made possible with the size of the dataset,
which provides us with the unique opportunity to do a detailed
architecture and hyperparameter analysis.
In essence, we perform an architecture and hyperparameter grid
search in pursuit of a lightweight model.

\section{Related work}
(TODO: izbaciti prvi paragraf?)
Research in the area of sentiment analysis has usually focused on determining a 
given text's polarity, be it negative or positive, due to the lack of suitable annotated data.
Even though the task of determining the emotion's intensity is challenging,
it does not undermine the problematic task that is affect detection.

A substantial number of approaches rely greatly on an underlying sentiment lexicon which, while being one of the most robust, is one of the most time-consuming methods. The trend regarding models based on word embeddings introduced by Mikhail et al. has no inclination of slowing down. 

\section{Dataset/Resources}
The dataset for all of the tasks was provided by \citep{mohammed-semeval} \emph{et al} (2018), both in English, Spanish and Arabic. Out of total 173236 samples, roughly 73\% is English, which is why the language of choice was English. Additional resources in form of word embeddigs and sentiment lexicons were provided by Stanford \citep{glove} and National Research Council in Canada \citep{nrcic}, respectively. 


\section{System Description}
In this section we present how our system performs the grid search.
The system has been implemented using the Keras deep learning library \citep{chollet2015keras}.

\subsection{Preprocessing}
For text preprocessing, we use the tokenizer tool provided in the Keras library.
The tweets are lowercased and the special characters and
punctuations are removed.
These characters include: %\textit{!"#\$%&()*+,-./:;<=>?@[\]^_`\{|\}~}.
After this normalization step, the tweets are tokenized using the Keras tokenizer.

\subsection{Input}
The words from the preprocessing step are mapped to word embeddings
and affect embeddings.
For word embeddings, we use the GloVe word vectors \citep{glove} 
which embed words into 200 dimensional vectors.
The affect embedding map words to the affect values which are
extracted from the NRC affect lexicon \citep{mohammed-semeval}.
These embeddings produce 4 dimensional word vectors where every
dimension represents an affect intensity value for the following emotions: \textit{anger, fear, joy, sadness}.
Finally, the two embeddings are concatenated into a single 204 dimension embedding.
These embeddings represent the model input.

\subsection{Architecture grid}
The system performs hyperparameter search for all architectures
given by the user. 
An architecture is defined by a list of layers from which
the model is built using the \textit{Sequential} Keras API.
The system fills every layer of the architecture with
arguments generated from the hyperparameter grid.
The sequential list structure of architectures does not
allow for more complex models with multiple inputs that can be
compiled using the \textit{Functional} Keras API.
But this was not our goal since we are searching for minimal models, 
although the system could be easily modified to satisfy such a need.

\subsection{Hyperparameter grid}
To perform hyperparameter search, the user needs to define
a list of possible values for chosen parameters of every type
of layer which is present in the architecture.
The system creates a cartesian product of hyperparameters
which are used to fill the layers of the specified architecture.

\begin{figure}
\begin{center}
\includegraphics[width=0.7\columnwidth]{images/architecture.pdf}
\caption{Model pipeline with an example architecture.}
\label{fig:figure1}
\end{center}
\end{figure}


\subsection{Training}

\section{Results}

\begin{table}
\caption{Task 1.a results (emotion intensity regression)}
\label{tab:narrow-table}
\begin{center}
\begin{tabular}{lllll}
\toprule
& joy & anger & fear & sadness \\
\midrule
EiTAKA    & 0.723  & 0.704 & 0.715 & 0.731 \\
Best Convnet & & & & \\
Small Convnet & 0.627 & 0.603 & 0.612 & 0.621 \\
SVM unigrams baseline & 0.526 & 0.525 & 0.575 & 0.453 \\
Random baseline & -0.018 & 0.024 & -0.058 & 0.020 \\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}



\section*{Acknowledgements}

If suitable, you can include the \textit{Acknowledgements} section before inserting the literature references  in order to thank those who helped you in any way to deliver the paper, but are not co-authors of the paper.

\bibliographystyle{tar2018}
\bibliography{tar2018} 

\end{document}

