{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic BoW model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from data import get_data\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(x):\n",
    "    # removes punctuation\n",
    "    x = re.sub(r'[.,\"!#/]+', ' ', x, flags=re.MULTILINE)\n",
    "    # removs \\\\n\n",
    "    x = re.sub(r'\\\\n', ' ', x, flags=re.MULTILINE)\n",
    "    # removes retweets (e.g. @FER)\n",
    "    x = re.sub(r'@\\w+', '', x, flags=re.MULTILINE)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_X(tweet, vocab):\n",
    "    # obsolete \n",
    "    # tweet = re.sub(r'[.,\"!#]+', ' ', tweet, flags=re.MULTILINE)\n",
    "    #tweet = re.sub(r'@\\w+', ' ', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # splits tags into 2, e.g. #happy -> #, happy\n",
    "    # TODO: tags could be useful for classification\n",
    "    tweet_toks = word_tokenize(tweet)\n",
    "    X_ = []\n",
    "    for w in vocab:\n",
    "        X_.append(1 if w in tweet_toks else 0)\n",
    "\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(tweets):\n",
    "    vocab = set()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    #stemmer = SnowballStemmer()\n",
    "    \n",
    "    clean_tweets = [cleanup(t) for t in tweets]\n",
    "    \n",
    "    for t in clean_tweets:\n",
    "        for w in word_tokenize(t):\n",
    "            vocab.add(w.strip())\n",
    "    \n",
    "    vocab -= set(stop_words)\n",
    "        \n",
    "    return sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Worry is a down payment on a problem you may never have'   Joyce Meyer    motivation  leadership  worry\n",
      "  (0, 487)\t0.8676948562339434\n",
      "  (0, 334)\t0.4970972102769805\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data('data/2018-E-c-En-train.txt')\n",
    "X_test, y_test = get_data('data/2018-E-c-En-dev.txt')\n",
    "\n",
    "X_train_clean = [cleanup(x) for x in X_train]\n",
    "X_test_clean = [cleanup(x) for x in X_test]\n",
    "\n",
    "#vocab = create_vocab(X)\n",
    "\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words='english')#, vocabulary=vocab)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=500)\n",
    "#reducer = TruncatedSVD(n_components=300, random_state=42)\n",
    "normalizer = Normalizer()\n",
    "clf = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "\n",
    "estimators = [('vectorize', vectorizer), ('normalize', normalizer), ('clf', clf)]\n",
    "\n",
    "print(X_train_clean[0])\n",
    "vectorizer.fit(X_train_clean)\n",
    "print(vectorizer.transform([X_train_clean[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Worry is a down payment on a problem you may never have'   Joyce Meyer    motivation  leadership  worry\n",
      "  (0, 487)\t0.8676948562339434\n",
      "  (0, 334)\t0.4970972102769805\n",
      "  (1, 408)\t0.5560172132022704\n",
      "  (1, 269)\t0.5264108866647862\n",
      "  (1, 268)\t0.4660600855948308\n",
      "  (1, 179)\t0.44331076418119336\n",
      "  (2, 491)\t0.7300553112533512\n",
      "  (2, 322)\t0.6833880614341843\n",
      "  (3, 244)\t0.5876297457788291\n",
      "  (3, 122)\t0.4800650057356455\n",
      "  (3, 6)\t0.6513285439345825\n",
      "  (4, 417)\t0.6434071602402384\n",
      "  (4, 302)\t0.765524151252978\n",
      "  (5, 376)\t1.0\n",
      "  (6, 426)\t1.0\n",
      "  (7, 435)\t0.3673646200452048\n",
      "  (7, 217)\t0.41166901455258914\n",
      "  (7, 139)\t0.7210813064173454\n",
      "  (7, 103)\t0.41906277325932556\n",
      "  (8, 449)\t0.5012580617338288\n",
      "  (8, 333)\t0.48405292781672216\n",
      "  (8, 197)\t0.5418983540235066\n",
      "  (8, 86)\t0.46987157024608195\n",
      "  (9, 162)\t1.0\n",
      "  (10, 377)\t0.5989722027390078\n",
      "  :\t:\n",
      "  (6831, 397)\t0.8255622334051632\n",
      "  (6831, 179)\t0.5643110833353167\n",
      "  (6832, 497)\t0.6922247929503843\n",
      "  (6832, 334)\t0.7216819493549756\n",
      "  (6833, 485)\t0.6949768682634876\n",
      "  (6833, 74)\t0.7190320942619147\n",
      "  (6834, 455)\t0.3516251560966404\n",
      "  (6834, 438)\t0.32665764514014395\n",
      "  (6834, 357)\t0.28706416220608855\n",
      "  (6834, 322)\t0.2955272128420946\n",
      "  (6834, 312)\t0.3516251560966404\n",
      "  (6834, 300)\t0.2810399020615307\n",
      "  (6834, 92)\t0.3386027406580992\n",
      "  (6834, 89)\t0.5316360341307681\n",
      "  (6835, 370)\t0.43514305387736096\n",
      "  (6835, 323)\t0.45129215953390384\n",
      "  (6835, 241)\t0.2562372821232803\n",
      "  (6835, 215)\t0.42020053416960473\n",
      "  (6835, 169)\t0.4096052680181105\n",
      "  (6835, 98)\t0.2637534799574092\n",
      "  (6835, 33)\t0.3569558823739027\n",
      "  (6836, 481)\t0.6207731088150017\n",
      "  (6836, 184)\t0.5262854992127336\n",
      "  (6836, 169)\t0.5810889094541057\n",
      "  (6837, 17)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data('data/2018-E-c-En-train.txt')\n",
    "X_test, y_test = get_data('data/2018-E-c-En-dev.txt')\n",
    "\n",
    "X_train_clean = np.array([cleanup(x) for x in X_train])\n",
    "X_test_clean = np.array([cleanup(x) for x in X_test])\n",
    "\n",
    "#vocab = create_vocab(X)\n",
    "\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 4), stop_words='english')#, vocabulary=vocab)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=500)\n",
    "#reducer = TruncatedSVD(n_components=300, random_state=42)\n",
    "normalizer = Normalizer()\n",
    "clf = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "\n",
    "estimators = [('vectorize', vectorizer), ('normalize', normalizer), ('clf', clf)]\n",
    "\n",
    "print(X_train_clean[0])\n",
    "vectorizer.fit(X_train_clean)\n",
    "print(vectorizer.transform(X_train_clean))\n",
    "#print(X_train_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19187358916478556\n",
      "0.002257336343115124\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(memory=None, steps=estimators).fit(X_train_clean, y_train)\n",
    "\n",
    "print(accuracy_score(y_test, pipe.predict(X_test_clean)))\n",
    "y_rand = np.array([np.round(np.random.uniform(0, 1, y_test.shape[1])) for _ in range(y_test.shape[0])])\n",
    "print(accuracy_score(y_test, y_rand))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-401f2a5dc5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/2018-EI-oc-En-anger-dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.training'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from data import get_data\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "X_train, y_train = get_data('data/2018-EI-oc-En-anger-dev.txt')\n",
    "\n",
    "print(to_categorical(y_train))\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
